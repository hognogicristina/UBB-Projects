{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3614e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee7e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#model1 = nn.Sequential(OrderedDict([\n",
    "#    ('hidden', nn.\n",
    "#]))\n",
    "\n",
    "model1 = nn.Sequential(OrderedDict([\n",
    "    ('hidden', nn.Linear(2, 2)), # 2 inputs, 2 outputs\n",
    "    ('activation', nn.ReLU()), # ReLU activation\n",
    "    ('output', nn.Linear(2, 1)) # 2 inputs, 1 output\n",
    "]))\n",
    "\n",
    "model2 = nn.Sequential(OrderedDict([\n",
    "    ('hidden1', nn.Linear(2, 3)),\n",
    "    ('activation1', nn.ReLU()),\n",
    "    ('hidden2', nn.Linear(3, 2)),\n",
    "    ('activation2', nn.ReLU()),\n",
    "    ('output', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "model3 = nn.Sequential(OrderedDict([\n",
    "    ('hidden1', nn.Linear(2, 4)),\n",
    "    ('activation1', nn.ReLU()),\n",
    "    ('hidden2', nn.Linear(4, 3)),\n",
    "    ('activation2', nn.ReLU()),\n",
    "    ('hidden3', nn.Linear(3, 2)),\n",
    "    ('activation3', nn.ReLU()),\n",
    "    ('output', nn.Linear(2, 1))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665ae958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (hidden): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Sequential(\n",
      "  (hidden1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (hidden2): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (activation2): ReLU()\n",
      "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Sequential(\n",
      "  (hidden1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (hidden2): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (activation2): ReLU()\n",
      "  (hidden3): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (activation3): ReLU()\n",
      "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model1)\n",
    "print(\"\\n\")\n",
    "print(model2)\n",
    "print(\"\\n\")\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26f0d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "#data_in = torch.tensor( ...\n",
    "\n",
    "data_in = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32) # 4x2 tensor\n",
    "\n",
    "print(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb16bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# data_target = torch.tensor( ...\n",
    "\n",
    "data_target = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32) # 4x1 tensor\n",
    "\n",
    "print(data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d920ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# criterion = \n",
    "# optimizer = \n",
    "\n",
    "criterion = nn.MSELoss() # mean squared error loss\n",
    "\n",
    "# stochastic gradient descent\n",
    "# learning rate = 0.1\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.1) \n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.1)\n",
    "optimizer3 = torch.optim.SGD(model3.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde91f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Accuracy: 0.0\n",
      "Model 2\n",
      "Accuracy: 0.0\n",
      "Model 3\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# Train the model\n",
    "\n",
    "# Train each model and print its accuracy\n",
    "\n",
    "for i, model in enumerate([model1, model2, model3]):\n",
    "    print(\"Model\", i + 1)\n",
    "    for epoch in range(10000):\n",
    "        # Forward pass\n",
    "        y_pred = model(data_in) # get the prediction\n",
    "        loss = criterion(y_pred, data_target) # calculate the loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer = [optimizer1, optimizer2, optimizer3][i] # get the correct optimizer\n",
    "        optimizer.zero_grad() # zero the gradient buffers\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step() # update weights\n",
    "\n",
    "        # Check if accuracy is 1.0 and print it\n",
    "        if loss.item() < 0.0001: # if loss is less than 0.0001\n",
    "            print(\"Accuracy:\", 1.0)\n",
    "            break\n",
    "    else:\n",
    "        print(\"Accuracy:\", 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff3ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# visualize the results\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(data_in) # output = 4x1 tensor\n",
    "    predicted = (output > 0).float() # predicted = 4x1 tensor\n",
    "    accuracy = (predicted == data_target).float().mean() # accuracy = 1x1 tensor\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy.item() * 100)) # accuracy.item() = 1x1 tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a7518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1.weight tensor([[-0.5113,  0.3812],\n",
      "        [ 0.3037,  0.1247],\n",
      "        [ 0.2543, -0.6859]])\n",
      "hidden1.bias tensor([-0.6020,  0.5021, -0.5232])\n",
      "hidden2.weight tensor([[-0.5070,  0.0735, -0.1961],\n",
      "        [-0.4641, -0.0362, -0.4622]])\n",
      "hidden2.bias tensor([ 0.6239, -0.0489])\n",
      "output.weight tensor([[ 0.1237, -0.5858]])\n",
      "output.bias tensor([0.4163])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# print model wights\n",
    "\n",
    "# Print the weights of the best model\n",
    "best_model = model2 # replace with the best model\n",
    "for name, param in best_model.named_parameters(): # iterate through the parameters\n",
    "    if param.requires_grad: # if the parameter requires gradient\n",
    "        print(name, param.data) # print the name and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cdf09ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adder(\n",
      "  (hidden): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (output): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (activation): Sigmoid()\n",
      ")\n",
      "Adder(\n",
      "  (hidden): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (output): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (activation): Sigmoid()\n",
      ")\n",
      "Adder(\n",
      "  (hidden): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (output): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (activation): Sigmoid()\n",
      ")\n",
      "Epoch 0 Loss: 0.25096720457077026\n",
      "Epoch 1000 Loss: 0.17934396862983704\n",
      "Epoch 2000 Loss: 0.0754101499915123\n",
      "Epoch 3000 Loss: 0.03455444052815437\n",
      "Epoch 4000 Loss: 0.01768871396780014\n",
      "Epoch 5000 Loss: 0.009792831726372242\n",
      "Epoch 6000 Loss: 0.005705052521079779\n",
      "Epoch 7000 Loss: 0.0034365025348961353\n",
      "Epoch 8000 Loss: 0.0021170189138501883\n",
      "Epoch 9000 Loss: 0.001324590528383851\n",
      "Accuracy of Model 1: 1.0\n",
      "Accuracy of Model 2: 0.5\n",
      "Accuracy of Model 3: 0.5\n",
      "hidden.weight tensor([[ 4.8211, -6.7707],\n",
      "        [ 6.9100, -4.9837],\n",
      "        [ 5.3163,  5.3013],\n",
      "        [-4.4876, -4.3654]])\n",
      "hidden.bias tensor([-1.9083,  1.9373, -0.4596, -0.4007])\n",
      "output.weight tensor([[ 7.4493, -7.3447,  3.4600, -3.0180]])\n",
      "output.bias tensor([0.4122])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Adder(nn.Module):\n",
    "    # define the model architecture\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 4) # 2 inputs, 4 outputs\n",
    "        self.output = nn.Linear(4, 1) # 4 inputs, 1 output\n",
    "        self.activation = nn.Sigmoid() # sigmoid activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward pass\n",
    "        x = self.activation(self.hidden(x)) # apply the activation function to the hidden layer\n",
    "        x = self.activation(self.output(x)) # apply the activation function to the output layer\n",
    "        return x\n",
    "\n",
    "# create three instances of the Adder model\n",
    "model1 = Adder()\n",
    "model2 = Adder()\n",
    "model3 = Adder()\n",
    "\n",
    "# print the models to inspect their architectures\n",
    "print(model1)\n",
    "print(model2)\n",
    "print(model3)\n",
    "\n",
    "# define the input and target tensors\n",
    "data_in = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float) # 4x2 tensor\n",
    "data_target = torch.tensor([[0], [1], [1], [0]], dtype=torch.float) # 4x1 tensor\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.MSELoss() # mean squared error loss\n",
    "optimizer = torch.optim.Adam(model1.parameters()) # Adam optimizer\n",
    "\n",
    "# train the first model\n",
    "for epoch in range(10000):\n",
    "    # forward pass\n",
    "    optimizer.zero_grad() # zero the gradient buffers\n",
    "    output = model1(data_in) # get the prediction\n",
    "    loss = criterion(output, data_target) # calculate the loss\n",
    "    loss.backward() # backward pass\n",
    "    optimizer.step() # update weights\n",
    "    if epoch % 1000 == 0: # print the loss every 1000 epochs\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item()}\") # loss.item() = 1x1 tensor\n",
    "\n",
    "# evaluate the performance of the three models\n",
    "with torch.no_grad():\n",
    "    # get the predictions\n",
    "    output1 = model1(data_in)\n",
    "    output2 = model2(data_in)\n",
    "    output3 = model3(data_in)\n",
    "\n",
    "    # calculate the accuracy\n",
    "    accuracy1 = ((output1 > 0.5) == data_target).sum().item() / data_target.shape[0] \n",
    "    accuracy2 = ((output2 > 0.5) == data_target).sum().item() / data_target.shape[0] \n",
    "    accuracy3 = ((output3 > 0.5) == data_target).sum().item() / data_target.shape[0]\n",
    "\n",
    "    print(f\"Accuracy of Model 1: {accuracy1}\")\n",
    "    print(f\"Accuracy of Model 2: {accuracy2}\")\n",
    "    print(f\"Accuracy of Model 3: {accuracy3}\")\n",
    "\n",
    "# print the weights of the first model\n",
    "for name, param in model1.named_parameters():\n",
    "    # only print the parameters that require gradients\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        \n",
    "        \n",
    "# In this example, we define a simple Adder class that inherits from nn.Module and contains two linear layers with sigmoid activations. \n",
    "# We then create three instances of this class and train the first one using the mean squared error loss and the Adam optimizer. \n",
    "# We evaluate the performance of all three models on the test data and print the weights of the first model.\n",
    "\n",
    "# Note that the architecture used in this example is relatively simple, with one hidden layer of 4 neurons. \n",
    "# You can experiment with different architectures to see if they improve performance. \n",
    "# Additionally, you can try different loss functions and optimizers to see how they affect training speed and accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
